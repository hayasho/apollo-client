---
title: Pagination
---

GraphQL servers provide selective access to a sprawling graph of data that GraphQL clients might request, containing countless entity objects and their fields. A typical GraphQL client needs only a small subgraph of this complete data graph, and uses GraphQL queries to fetch all and only the fields it needs.

While requesting the specific fields your application needs is an important technique for preventing over-fetching, it's not the only way to reduce response size. A single list field can hold arbitrarily many elements, possibly infinitely many. If you need some of that data but not all of it, what can you do? Omitting the field from your query is not an option, but neither is requesting the entire list.

To solve this problem, GraphQL servers often let clients avoid fetching entire lists of data at once, by allowing the lists to be split up into separate chunks or _pages_, which can be requested incrementally using field arguments like `offset`, `cursor`, and/or `limit`.

_Pagination_ is the term for this general technique of incrementally requesting large collections of data (usually lists) in smaller pages, using field arguments. Pagination comes in a number of different flavors: offset-based, cursor-based, page-number-based, forwards, backwards, displayed in discrete pages or via infinite scroll, and so on, each calling for a slightly different set of field arguments. As this diversity suggests, pagination is not an automatic feature of list-valued fields in GraphQL, but GraphQL server tools make pagination straightforward to set up, and so it has become a ubiquitous pattern.

Rather than attempting to standardize on one style of pagination to serve all use cases, Apollo Client enables any and all such patterns through the flexible `InMemoryCache` field policy API, which  allows you to configure the client-side behavior of fields across your graph in a consistent, declarative, reusable way, in one place, rather than reproducing custom field logic wherever the field is used throughout your application. You can get started by copying and pasting example code from this documentation, but you will no doubt end up adapting and expanding your field policies to suit your specific needs. Whenever you need to apply similar logic elsewhere, remember that you can encapsulate common policies in reusable policy-generating functions.

In a way, the pagination pattern is a microcosm for GraphQL as a whole, since GraphQL can be described as a strongly-typed pagination interface for graph-structured data. Since pagination pulls together so many different pieces of GraphQL, we hope this documentation makes those parts click together for you in a fresh way.

## Combining pages with `merge` functions

While it is possible to store individual pages of list data separately on the client, application code that consumes the data tends to be simpler if the client combines all the pages it has received so far into a single list, using field arguments to guide how new data is spliced together with existing data.

Suppose your schema defines the following types:
```gql
extend type Query {
  feed(
    type: FeedType!,
    offset: Int,
    limit: Int,
  ): [FeedItem!]
}

enum FeedType {
  PERSONAL
  PUBLIC
}

type FeedItem {
  id: String!
  # ...
}
```

The query you use to fetch data for the `Query.feed` field might look something like this:
```ts
const FEED_QUERY = gql`
  query Feed($type: FeedType!, $offset: Int, $limit: Int) {
    feed(type: $type, offset: $offset, limit: $limit) {
      id
      # ... other FeedItem fields
    }
  }
`;
```

Without input from you, `InMemoryCache` has no way of knowing how to interpret these field arguments, so it must assume they are all important. For example, imagine writing two consecutive pages into a cache that does not know about your pagination system:
```ts
const cache = new InMemoryCache;

cache.writeQuery({
  query: FEED_QUERY,
  variables: {
    type: "PERSONAL",
    offset: 0,
    limit: 2,
  },
  data: {
    feed: [
      { __typename: "FeedItem", id: 1 },
      { __typename: "FeedItem", id: 2 },
    ]
  }
});

cache.writeQuery({
  query: FEED_QUERY,
  variables: {
    type: "PERSONAL",
    offset: 2, // Changed!
    limit: 2,
  },
  data: {
    feed: [
      { __typename: "FeedItem", id: 3 },
      { __typename: "FeedItem", id: 4 },
    ]
  }
});
```
The cache stores those two results separately by default, using keys derived from both the field name (`feed`) and the serialized arguments (`limit`, `offset`, and `type`):
```js
expect(cache.extract()).toEqual({
  // Internal, normalized InMemoryCache data:
  "FeedItem:1": { __typename: "FeedItem", id: 1 },
  "FeedItem:2": { __typename: "FeedItem", id: 2 },
  "FeedItem:3": { __typename: "FeedItem", id: 3 },
  "FeedItem:4": { __typename: "FeedItem", id: 4 },
  "ROOT_QUERY": {
    __typename: "Query",
    // Notice we end up with two separate pages, one per unique
    // combination of Query.feed field arguments.
    'feed({"limit":2,"offset":0,"type":"PERSONAL"})': [
      { __ref: "FeedItem:1" },
      { __ref: "FeedItem:2" },
    ],
    'feed({"limit":2,"offset":2,"type":"PERSONAL"})': [
      { __ref: "FeedItem:3" },
      { __ref: "FeedItem:4" },
    ],
  },
});
```
A simplistic way to accumulate a single list instead of storing separate lists is to concatenate the results together, using a `merge` function for the `Query.feed` field:
```js
const cache = new InMemoryCache({
  typePolicies: {
    Query: {
      fields: {
        feed: {
          // Stop using limit and offset to differentiate field keys,
          // but continue using type.
          keyArgs: ["type"],
          // If there is no existing value for this field, existing
          // will be undefined.
          merge(existing = [], incoming) {
            return [...existing, ...incoming];
          },
        }
      }
    }
  }
})
```
Now that the cache has a strategy for combining incoming pages with existing data, its internal representation will be a bit more compact:
```js
expect(cache.extract()).toEqual({
  "FeedItem:1": { __typename: "FeedItem", id: 1 },
  "FeedItem:2": { __typename: "FeedItem", id: 2 },
  "FeedItem:3": { __typename: "FeedItem", id: 3 },
  "FeedItem:4": { __typename: "FeedItem", id: 4 },
  "ROOT_QUERY": {
    __typename: "Query",
    // One consolidated list of Query.feed data (per type):
    'feed({"type":"PERSONAL"})': [
      { __ref: "FeedItem:1" },
      { __ref: "FeedItem:2" },
      { __ref: "FeedItem:3" },
      { __ref: "FeedItem:4" },
    ],
  },
});
```
However, this simple strategy makes some risky assumptions about the order of the written pages, because it ignores the `offset` and `limit` arguments. In order to handle those arguments correctly, your `merge` function should use `options.args` to decide where to put the incoming data:
```js
const cache = new InMemoryCache({
  typePolicies: {
    Query: {
      fields: {
        feed: {
          keyArgs: ["type"],
          merge(existing, incoming, { args: { offset = 0 }}) {
            const merged = existing ? existing.slice(0) : [];
            for (let i = 0; i < incoming.length; ++i) {
              merged[offset + i] = incoming[i];
            }
            return merged;
          },
        },
      },
    },
  },
});
```
This logic handles sequential page writes the same way the concatenation strategy would, but it can also tolerate repeated, overlapping, or out-of-order writes, without duplicating any list items.

As we mentioned in the introduction above, common field policy patterns can be abstracted away into helper functions, such as the `offsetLimitPagination` function that `@apollo/client/utilities` provides:
```js
import { offsetLimitPagination } from "@apollo/client/utilities"

const cache = new InMemoryCache({
  typePolicies: {
    Query: {
      fields: {
        feed: offsetLimitPagination(["type"]),
      },
    },
  },
});
```

## Two kinds of `read` function

TODO

## Using `fetchMore`

In Apollo, the easiest way to do pagination is with a function called [`fetchMore`](../caching/advanced-topics/#incremental-loading-fetchmore), which is included in the result object returned by the `useQuery` Hook. This basically allows you to do a new GraphQL query and merge the result into the original result.

You can specify what query and variables to use for the new query, and how to merge the new query result with the existing data on the client. How exactly you do that will determine what kind of pagination you are implementing.

## Offset-based

Offset-based pagination — also called numbered pages — is a very common pattern, found on many websites, because it is usually the easiest to implement on the backend. In SQL for example, numbered pages can easily be generated by using [OFFSET and LIMIT](https://www.postgresql.org/docs/8.2/static/queries-limit.html).

```jsx
const FEED_QUERY = gql`
  query Feed($type: FeedType!, $offset: Int, $limit: Int) {
    currentUser {
      login
    }
    feed(type: $type, offset: $offset, limit: $limit) {
      id
      # ...
    }
  }
`;

const FeedData({ match }) {
  const { data, fetchMore } = useQuery(
    FEED_QUERY,
    {
      variables: {
        type: match.params.type.toUpperCase() || "TOP",
        offset: 0,
        limit: 10
      },
      fetchPolicy: "cache-and-network"
    }
  );

  return (
    <Feed
      entries={data.feed || []}
      onLoadMore={() =>
        fetchMore({
          variables: {
            offset: data.feed.length
          },
          updateQuery: (prev, { fetchMoreResult }) => {
            if (!fetchMoreResult) return prev;
            return Object.assign({}, prev, {
              feed: [...prev.feed, ...fetchMoreResult.feed]
            });
          }
        })
      }
    />
  );
}
```

As you can see, `fetchMore` is accessible through the `useQuery` Hook result object. By default, `fetchMore` will use the original `query`, so we just pass in new variables. Once the new data is returned from the server, the `updateQuery` function is used to merge it with the existing data, which will cause a re-render of your UI component with an expanded list.

The above approach works great for limit/offset pagination. One downside of pagination with numbered pages or offsets is that an item can be skipped or returned twice when items are inserted into or removed from the list at the same time. That can be avoided with cursor-based pagination.

Note that in order for the UI component to receive an updated `loading` prop after `fetchMore` is called, you must set `notifyOnNetworkStatusChange` to `true` in your Query component's props.

## Cursor-based

In cursor-based pagination, a "cursor" is used to keep track of where in the data set the next items should be fetched from. Sometimes the cursor can be quite simple and just refer to the ID of the last object fetched, but in some cases — for example lists sorted according to some criteria — the cursor needs to encode the sorting criteria in addition to the ID of the last object fetched.

Implementing cursor-based pagination on the client isn't all that different from offset-based pagination, but instead of using an absolute offset, we keep a reference to the last object fetched and information about the sort order used.

In the example below, we use a `fetchMore` query to continuously load new comments, which will be prepended to the list. The cursor to be used in the `fetchMore` query is provided in the initial server response, and is updated whenever more data is fetched.

```jsx
const MORE_COMMENTS_QUERY = gql`
  query MoreComments($cursor: String) {
    moreComments(cursor: $cursor) {
      cursor
      comments {
        author
        text
      }
    }
  }
`;

function CommentsWithData() {
  const { data: { moreComments: {comments, cursor} }, loading, fetchMore } = useQuery(
    MORE_COMMENTS_QUERY
  );

  return (
    <Comments
      entries={comments || []}
      onLoadMore={() =>
        fetchMore({
          // note this is a different query than the one used in the
          // Query component
          query: MORE_COMMENTS_QUERY,
          variables: { cursor: cursor },
          updateQuery: (previousResult, { fetchMoreResult }) => {
            const previousEntry = previousResult.entry;
            const newComments = fetchMoreResult.moreComments.comments;
            const newCursor = fetchMoreResult.moreComments.cursor;

            return {
              // By returning `cursor` here, we update the `fetchMore` function
              // to the new cursor.
              cursor: newCursor,
              entry: {
                // Put the new comments in the front of the list
                comments: [...newComments, ...previousEntry.comments]
              },
              __typename: previousEntry.__typename
            };
          }
        })
      }
    />
  );
}
```

## Relay-style cursor pagination

Relay, another popular GraphQL client, is opinionated about the input and output of paginated queries, so people sometimes build their server's pagination model around Relay's needs. If you have a server that is designed to work with the [Relay Cursor Connections](https://facebook.github.io/relay/graphql/connections.htm) spec, you can also call that server from Apollo Client with no problems.

Using Relay-style cursors is very similar to basic cursor-based pagination.  The main difference is in the format of the query response which affects where you get the cursor.

Relay provides a `pageInfo` object on the returned cursor connection which contains the cursor of the first and last items returned as the properties `startCursor` and `endCursor` respectively.  This object also contains a boolean property `hasNextPage` which can be used to determine if there are more results available.

The following example specifies a request of 10 items at a time and that results should start after the provided `cursor`.  If `null` is passed for the cursor relay will ignore it and provide results starting from the beginning of the data set which allows the use of the same query for both initial and subsequent requests.

```jsx
const COMMENTS_QUERY = gql`
  query Comments($cursor: String) {
    Comments(first: 10, after: $cursor) {
      edges {
        node {
          author
          text
        }
      }
      pageInfo {
        endCursor
        hasNextPage
      }
    }
  }
`;

function CommentsWithData() {
  const { data: { Comments: comments }, loading, fetchMore } = useQuery(
    COMMENTS_QUERY
  );

  return (
    <Comments
      entries={comments || []}
      onLoadMore={() =>
        fetchMore({
          variables: {
            cursor: comments.pageInfo.endCursor
          },
          updateQuery: (previousResult, { fetchMoreResult }) => {
            const newEdges = fetchMoreResult.comments.edges;
            const pageInfo = fetchMoreResult.comments.pageInfo;

            return newEdges.length
              ? {
                  // Put the new comments at the end of the list and update `pageInfo`
                  // so we have the new `endCursor` and `hasNextPage` values
                  comments: {
                    __typename: previousResult.comments.__typename,
                    edges: [...previousResult.comments.edges, ...newEdges],
                    pageInfo
                  }
                }
              : previousResult;
          }
        })
      }
    />
  );
}
```

## The `@connection` directive

When using paginated queries, results from accumulated queries can be hard to find in the store, as the parameters passed to the query are used to determine the default store key but are usually not known outside the piece of code that executes the query. This is problematic for imperative store updates, as there is no stable store key for updates to target. To direct Apollo Client to use a stable store key for paginated queries, you can use the optional `@connection` directive to specify a store key for parts of your queries. For example, if we wanted to have a stable store key for the feed query earlier, we could adjust our query to use the `@connection` directive:

```js
const FEED_QUERY = gql`
  query Feed($type: FeedType!, $offset: Int, $limit: Int) {
    currentUser {
      login
    }
    feed(type: $type, offset: $offset, limit: $limit) @connection(key: "feed", filter: ["type"]) {
      id
      # ...
    }
  }
`;
```

This would result in the accumulated feed in every query or `fetchMore` being placed in the store under the `feed` key, which we could later use for imperative store updates. In this example, we also use the `@connection` directive's optional `filter` argument, which allows us to include some arguments of the query in the store key. In this case, we want to include the `type` query argument in the store key, which results in multiple store values that accumulate pages from each type of feed.
